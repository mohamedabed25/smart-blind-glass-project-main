{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e94700e-f909-45d1-9c07-c5aff9970328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import face_recognition\n",
    "import pyttsx3\n",
    "import time\n",
    "import cv2\n",
    "import pytesseract\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import speech_recognition as sr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2437156-14f2-4299-8fdc-60a74a70470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces():\n",
    "    # Initialize text-to-speech engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Load known faces and names\n",
    "    known_faces = ['photos/mohamed_elhalak.jpg']\n",
    "    known_names = ['mohamed tarek']\n",
    "    \n",
    "    known_images = [cv2.imread(img) for img in known_faces]\n",
    "    encodings_known = [face_recognition.face_encodings(img)[0] for img in known_images]\n",
    "\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    face_announced = False\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame from webcam\")\n",
    "            break\n",
    "\n",
    "        img_resized = cv2.resize(img, (0, 0), None, 0.25, 0.25)\n",
    "        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        face_locations = face_recognition.face_locations(img_resized)\n",
    "        encodings_current = face_recognition.face_encodings(img_resized, face_locations)\n",
    "\n",
    "        if len(encodings_current) != 0:\n",
    "            for face_encoding, face_location in zip(encodings_current, face_locations):\n",
    "                matches = face_recognition.compare_faces(encodings_known, face_encoding)\n",
    "                face_distances = face_recognition.face_distance(encodings_known, face_encoding)\n",
    "                match_index = np.argmin(face_distances)\n",
    "\n",
    "                if matches[match_index] and not face_announced:\n",
    "                    name = known_names[match_index]\n",
    "                    engine.say(name)\n",
    "                    engine.runAndWait()\n",
    "                    face_announced = True\n",
    "                \n",
    "\n",
    "                top, right, bottom, left = face_location\n",
    "                top *= 4\n",
    "                right *= 4\n",
    "                bottom *= 4\n",
    "                left *= 4\n",
    "                cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                cv2.putText(img, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        else:\n",
    "            face_announced = False\n",
    "\n",
    "        cv2.imshow('Face Recognition', img)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b900d-29fb-4450-bf37-b534579d98cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25993124-9107-4a60-8cc3-7b37f7261e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_objects():\n",
    "    engine = pyttsx3.init()\n",
    "    \n",
    "    # Load class names\n",
    "    with open('coco.names', 'rt') as f:\n",
    "        classNames = f.read().rstrip('\\n').split('\\n')\n",
    "        \n",
    "    # Load model\n",
    "    net = cv2.dnn_DetectionModel('frozen_inference_graph.pb', 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt')\n",
    "    net.setInputSize(320, 230)\n",
    "    net.setInputScale(1.0 / 127.5)\n",
    "    net.setInputMean((127.5, 127.5, 127.5))\n",
    "    net.setInputSwapRB(True)\n",
    "    \n",
    "    # Webcam capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    detected_objects = set()\n",
    "    last_announced_objects = set()\n",
    "    \n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect objects\n",
    "        classIds, confs, bbox = net.detect(img, confThreshold=0.5)\n",
    "        detected_objects_in_frame = set()\n",
    "        \n",
    "        # Process detections\n",
    "        if len(classIds) != 0:\n",
    "            for classId, confidence, box in zip(classIds.flatten(), confs.flatten(), bbox):\n",
    "                className = classNames[classId - 1]\n",
    "                cv2.rectangle(img, box, color=(0, 255, 0), thickness=2)\n",
    "                cv2.putText(img, className, (box[0] + 10, box[1] + 20), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), thickness=2)\n",
    "                detected_objects_in_frame.add(className)\n",
    "                \n",
    "                # Announce new objects in the frame\n",
    "                if className not in detected_objects:\n",
    "                    engine.say(f\"{className}\")\n",
    "                    engine.runAndWait()\n",
    "\n",
    "            # Announce objects that exited the frame\n",
    "            for obj in detected_objects - detected_objects_in_frame:\n",
    "                if obj not in last_announced_objects:  # Avoid duplicate announcements\n",
    "                    engine.say(f\"{obj} has left the frame.\")\n",
    "                    engine.runAndWait()\n",
    "                    last_announced_objects.add(obj)\n",
    "                \n",
    "            # Reset last announced objects to track changes per frame\n",
    "            last_announced_objects = detected_objects_in_frame.copy()\n",
    "            \n",
    "        detected_objects = detected_objects_in_frame\n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow('Object Detection', img)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e6e7c-d288-4d2a-8eb2-7154753df3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee28fa-2dfd-4fb7-adc6-90b9f16e34d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1f4925-f03c-4639-b255-919346d611ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recognize_text():\n",
    "    while True:  # Loop until a valid input is provided\n",
    "        # Prompt user for input\n",
    "        mode = input(\"Enter 'live' for live camera input, 'static' for a static image, or 'pdf' for a PDF file: \").strip().lower()\n",
    "        \n",
    "        if mode == 'static':\n",
    "            image_path = input(\"Please enter the path to the image file: \").strip()\n",
    "            # Load image from the provided path\n",
    "            frame = cv2.imread(image_path)\n",
    "            if frame is None:\n",
    "                print(f\"Error loading image from {image_path}. Please check the file path.\")\n",
    "                continue  # Prompt for input again\n",
    "\n",
    "        elif mode == 'live':\n",
    "            # Delay for camera initialization\n",
    "            time.sleep(2)\n",
    "            cap = cv2.VideoCapture(0)\n",
    "            time.sleep(2)  # Wait for the camera to be ready\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "            \n",
    "            if not ret:\n",
    "                print(\"Error capturing image from camera.\")\n",
    "                continue  # Prompt for input again\n",
    "            \n",
    "            # Optional: Add a delay before processing the image\n",
    "            time.sleep(2)  # Delay to ensure the model can read the photo\n",
    "\n",
    "        elif mode == 'pdf':\n",
    "            pdf_path = input(\"Please enter the path to the PDF file: \").strip()\n",
    "            # Open the PDF file\n",
    "            doc = fitz.open(pdf_path)\n",
    "            extracted_text = \"\"\n",
    "            for page in doc:\n",
    "                # Render each page to an image\n",
    "                pix = page.get_pixmap()\n",
    "                img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, -1)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)  # Handle RGBA to BGR conversion\n",
    "                # Recognize text in the rendered page\n",
    "                extracted_text += pytesseract.image_to_string(img)\n",
    "            doc.close()\n",
    "            print(\"Extracted Text from PDF:\", extracted_text)\n",
    "            # Convert extracted text to audio\n",
    "            tts = gTTS(text=extracted_text, lang='en')\n",
    "            tts.save('output_audio.mp3')\n",
    "            os.system('start output_audio.mp3')  # Adjust for OS if needed\n",
    "            return  # End the function after processing the PDF\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid option. Please enter 'live', 'static', or 'pdf'.\")\n",
    "            continue  # Prompt for input again\n",
    "\n",
    "        # Recognize text in the captured or loaded frame\n",
    "        extracted_text = pytesseract.image_to_string(frame)\n",
    "        print(\"Extracted Text:\", extracted_text)\n",
    "        \n",
    "        # Convert extracted text to audio\n",
    "        tts = gTTS(text=extracted_text, lang='en')\n",
    "        tts.save('output_audio.mp3')\n",
    "        \n",
    "        # Play the audio file\n",
    "        os.system('start output_audio.mp3')  # Adjust for OS if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40e192ef-4191-48f0-99e1-881e66b86312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speech():\n",
    "    \"\"\"Recognize speech from the microphone.\"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio)\n",
    "            print(\"You said:\", command)\n",
    "            return command.lower()\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I could not understand the audio.\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51024145-9839-4c0f-ab09-9b90aea67be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Welcome! Would you like to (s)peak or (t)ype your commands?\")\n",
    "    input_method = input(\"Enter 's' for speech or 't' for typing: \").strip().lower()\n",
    "\n",
    "    while True:\n",
    "        if input_method == 's':\n",
    "            command = recognize_speech()\n",
    "        elif input_method == 't':\n",
    "            print(\"\"\"\n",
    "            read? for -- 'reed: pdf,images,live_vid' -- \n",
    "            who?  for -- 'read:  faces' -- \n",
    "            what? for -- 'read: objects' -- \n",
    "            exit? for -- 'exit our project' -- \n",
    "            \"\"\")\n",
    "            command = input(\"Please type wich model u need to run\").strip().lower()\n",
    "        else:\n",
    "            print(\"Invalid option. Please enter 's' for speech or 't' for typing.\")\n",
    "            continue\n",
    "\n",
    "        if not command:\n",
    "            continue\n",
    "\n",
    "        if any(word in command for word in [\"voice\", \"read\"]):\n",
    "            recognize_text()\n",
    "        elif any(word in command for word in [\"face\", \"who\"]):\n",
    "            recognize_faces()\n",
    "        elif any(word in command for word in [\"object\", \"what\"]):\n",
    "            detect_objects()\n",
    "        elif command == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Command not recognized. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4fe02a6-0fa2-44e8-a7d2-58269a3fef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! Would you like to (s)peak or (t)ype your commands?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 's' for speech or 't' for typing:  t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            read? for -- 'reed: pdf,images,live_vid' -- \n",
      "            who?  for -- 'read:  faces' -- \n",
      "            what? for -- 'read: objects' -- \n",
      "            exit? for -- 'exit our project' -- \n",
      "            \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type wich model u need to run read\n",
      "Enter 'live' for live camera input, 'static' for a static image, or 'pdf' for a PDF file:  pdf\n",
      "Please enter the path to the PDF file:  pdf/sheet_pdf.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text from PDF: 21 ~ | &\n",
      "\n",
      "A 8 c OO e F sc\n",
      "‘Client Name Project Type ate Complete Hours Spent Amount Billec Hourly Rate\n",
      "2 Karma Security Video Creation 6/30/2024 22 $ 1,100.00 $ 50.00\n",
      "3 Elite Motors Proofreading 5/31/2024 2$ 120.00 $ 60.00\n",
      "4 Sunshine Navig: Coaching 5/20/2024 14$ 742.00 $ 53.00\n",
      "5 IceCap Producti Copy Editing 4/8/2024 1 $ 462.00 $ 42.00\n",
      "© Pumpkin Naviga Ghostwriting 7/3/2024 8 $ 504.00 $ 63.00\n",
      "7 Summit Electron Coaching 3/18/2024 33 $ 2,112.00 $ 64.00\n",
      "8 — Grizzly Limited Ghostwriting 6/9/2024 14$ 630.00 $ 45.00\n",
      "8 Thor Records Video Creation 71612024 23 $ 1,311.00 $ 57.00\n",
      "10 Hurricane Netwc Ghostwriting 5/30/2024 20 $ 1,240.00 $ 62.00\n",
      "\n",
      "\n",
      "\n",
      "            read? for -- 'reed: pdf,images,live_vid' -- \n",
      "            who?  for -- 'read:  faces' -- \n",
      "            what? for -- 'read: objects' -- \n",
      "            exit? for -- 'exit our project' -- \n",
      "            \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type wich model u need to run who\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            read? for -- 'reed: pdf,images,live_vid' -- \n",
      "            who?  for -- 'read:  faces' -- \n",
      "            what? for -- 'read: objects' -- \n",
      "            exit? for -- 'exit our project' -- \n",
      "            \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type wich model u need to run wha\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command not recognized. Please try again.\n",
      "\n",
      "            read? for -- 'reed: pdf,images,live_vid' -- \n",
      "            who?  for -- 'read:  faces' -- \n",
      "            what? for -- 'read: objects' -- \n",
      "            exit? for -- 'exit our project' -- \n",
      "            \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type wich model u need to run what\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            read? for -- 'reed: pdf,images,live_vid' -- \n",
      "            who?  for -- 'read:  faces' -- \n",
      "            what? for -- 'read: objects' -- \n",
      "            exit? for -- 'exit our project' -- \n",
      "            \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type wich model u need to run exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5905c-1950-45ed-a8dd-9c474f29e39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
