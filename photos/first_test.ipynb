{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e94700e-f909-45d1-9c07-c5aff9970328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import pyttsx3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2437156-14f2-4299-8fdc-60a74a70470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces():\n",
    "    # Initialize text-to-speech engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Load known faces and names\n",
    "    known_faces = ['photos/mohamed_elhalak.jpg']\n",
    "    known_names = ['mohamed tarek']\n",
    "    \n",
    "    known_images = [cv2.imread(img) for img in known_faces]\n",
    "    encodings_known = [face_recognition.face_encodings(img)[0] for img in known_images]\n",
    "\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    face_announced = False\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame from webcam\")\n",
    "            break\n",
    "\n",
    "        img_resized = cv2.resize(img, (0, 0), None, 0.25, 0.25)\n",
    "        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        face_locations = face_recognition.face_locations(img_resized)\n",
    "        encodings_current = face_recognition.face_encodings(img_resized, face_locations)\n",
    "\n",
    "        if len(encodings_current) != 0:\n",
    "            for face_encoding, face_location in zip(encodings_current, face_locations):\n",
    "                matches = face_recognition.compare_faces(encodings_known, face_encoding)\n",
    "                face_distances = face_recognition.face_distance(encodings_known, face_encoding)\n",
    "                match_index = np.argmin(face_distances)\n",
    "\n",
    "                if matches[match_index] and not face_announced:\n",
    "                    name = known_names[match_index]\n",
    "                    engine.say(name)\n",
    "                    engine.runAndWait()\n",
    "                    face_announced = True\n",
    "\n",
    "                top, right, bottom, left = face_location\n",
    "                top *= 4\n",
    "                right *= 4\n",
    "                bottom *= 4\n",
    "                left *= 4\n",
    "                cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                cv2.putText(img, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        else:\n",
    "            face_announced = False\n",
    "\n",
    "        cv2.imshow('Face Recognition', img)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b900d-29fb-4450-bf37-b534579d98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25993124-9107-4a60-8cc3-7b37f7261e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_objects():\n",
    "    engine = pyttsx3.init()\n",
    "    \n",
    "    # Load class names\n",
    "    with open('coco.names', 'rt') as f:\n",
    "        classNames = f.read().rstrip('\\n').split('\\n')\n",
    "        \n",
    "    # Load model\n",
    "    net = cv2.dnn_DetectionModel('frozen_inference_graph.pb', 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt')\n",
    "    net.setInputSize(320, 230)\n",
    "    net.setInputScale(1.0 / 127.5)\n",
    "    net.setInputMean((127.5, 127.5, 127.5))\n",
    "    net.setInputSwapRB(True)\n",
    "    \n",
    "    # Webcam capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    detected_objects = set()\n",
    "    last_announced_objects = set()\n",
    "    \n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect objects\n",
    "        classIds, confs, bbox = net.detect(img, confThreshold=0.5)\n",
    "        detected_objects_in_frame = set()\n",
    "        \n",
    "        # Process detections\n",
    "        if len(classIds) != 0:\n",
    "            for classId, confidence, box in zip(classIds.flatten(), confs.flatten(), bbox):\n",
    "                className = classNames[classId - 1]\n",
    "                cv2.rectangle(img, box, color=(0, 255, 0), thickness=2)\n",
    "                cv2.putText(img, className, (box[0] + 10, box[1] + 20), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), thickness=2)\n",
    "                detected_objects_in_frame.add(className)\n",
    "                \n",
    "                # Announce new objects in the frame\n",
    "                if className not in detected_objects:\n",
    "                    engine.say(f\"{className}\")\n",
    "                    engine.runAndWait()\n",
    "\n",
    "            # Announce objects that exited the frame\n",
    "            for obj in detected_objects - detected_objects_in_frame:\n",
    "                if obj not in last_announced_objects:  # Avoid duplicate announcements\n",
    "                    engine.say(f\"{obj} has left the frame.\")\n",
    "                    engine.runAndWait()\n",
    "                    last_announced_objects.add(obj)\n",
    "                \n",
    "            # Reset last announced objects to track changes per frame\n",
    "            last_announced_objects = detected_objects_in_frame.copy()\n",
    "            \n",
    "        detected_objects = detected_objects_in_frame\n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow('Object Detection', img)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669e6e7c-d288-4d2a-8eb2-7154753df3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ee28fa-2dfd-4fb7-adc6-90b9f16e34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import pytesseract\n",
    "from gtts import gTTS\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef1f4925-f03c-4639-b255-919346d611ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recognize_text():\n",
    "    # Delay for camera initialization\n",
    "    time.sleep(2)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    time.sleep(2)  # Wait for the camera to be ready\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error capturing image from camera.\")\n",
    "        return\n",
    "    \n",
    "    # Optional: Add a delay before processing the image\n",
    "    time.sleep(2)  # Delay to ensure the model can read the photo\n",
    "    \n",
    "    # Recognize text in the captured frame\n",
    "    extracted_text = pytesseract.image_to_string(frame)\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "    \n",
    "    # Convert extracted text to audio\n",
    "    tts = gTTS(text=extracted_text, lang='en')\n",
    "    tts.save('output_audio.mp3')\n",
    "    \n",
    "    # Play the audio file\n",
    "    os.system('start output_audio.mp3')  # Adjust for OS if needed (e.g., 'open' for Mac, 'xdg-open' for Linux)\n",
    "\n",
    "# Call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e192ef-4191-48f0-99e1-881e66b86312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: Client Name Project Type Date Completed Hours Spent Amount Bi\n",
      "deo Creation 6/30/2024 2 $\n",
      "\n",
      "Karma Security\n",
      "\n",
      "5/31/2024 2$\n",
      "\n",
      "5/20/2024 14 $\n",
      "\n",
      "4/8/2024 1 $\n",
      "\n",
      "7/3/2024 8s\n",
      "\n",
      "3/18/2024 33 $ 2\n",
      "\n",
      "6/9/2024 14 $ 63\n",
      ": 7/16/2024 23 $\n",
      "\n",
      "5/30/2024 20\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recognize_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51024145-9839-4c0f-ab09-9b90aea67be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
